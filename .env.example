# LLM Provider Configuration
# Default provider: ollama, openai, or anthropic
LLM_PROVIDER=ollama

# Default model (provider-specific)
# For Ollama: llama3.2, mistral, phi3, etc.
# For OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo, etc.
# For Anthropic: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, etc.
LLM_MODEL=llama3.2

# LLM parameters
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2000

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_ORG=your-org-id-here  # Optional
OPENAI_BASE_URL=  # Optional, for Azure OpenAI or proxies

# Anthropic Configuration
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_BASE_URL=  # Optional, for custom endpoints

# API Server
API_HOST=0.0.0.0
API_PORT=8000

# Database URLs
REDIS_URL=redis://localhost:6379
DATABASE_URL=postgresql://user:pass@localhost/memory_agent

# CORS Origins (comma-separated)
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# API Keys (comma-separated, optional)
API_KEYS=